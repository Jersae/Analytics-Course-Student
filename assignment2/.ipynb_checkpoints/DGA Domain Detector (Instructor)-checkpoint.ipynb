{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise to detect Algorithmically Generated Domain Names.\n",
    "In this notebook we're going to use some great python modules to explore, understand and classify domains as being 'legit' or having a high probability of being generated by a DGA (Domain Generation Algorithm). We have 'legit' in quotes as we're using the domains in Alexa as the 'legit' set. The primary motivation is to explore the nexus of IPython, Pandas and scikit-learn with DGA classification as a vehicle for that exploration. The exercise intentionally shows common missteps, warts in the data, paths that didn't work out that well and results that could definitely be improved upon. In general capturing what worked and what didn't is not only more realistic but often much more informative. :)\n",
    "\n",
    "### Python Modules Used:\n",
    "- Pandas: Python Data Analysis Library (http://pandas.pydata.org)\n",
    "- Scikit Learn (http://scikit-learn.org) Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "- Matplotlib:  Python 2D plotting library (http://matplotlib.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "# Set default pylab stuff\n",
    "pylab.rcParams['figure.figsize'] = (14.0, 5.0)\n",
    "pylab.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Version 0.12.0 of Pandas has a DeprecationWarning about Height blah that I'm ignoring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Legitimate Domain Set with Pandas Dataframe.\n",
    "In any programming problem, it is important to use the right data structure for holding data. For the data scientist, Pandas dataframe is the data structure of choice for data wrangling and pre-processing.\n",
    "\n",
    "A Pandas Dataframe is a 2-Dimensional size mutable tabular data structure. Just understand it like a table with headers and row numbers from which we can manipulate the data.\n",
    "\n",
    "The code below loads a csv file **'alexa_100k.csv'** into a dataframe called **'alexa_dataframe'**. In the same line, we also set the **_header=None_** since the csv file has no header. We specify the column names to be **'rank'** and **'uri'**.\n",
    "\n",
    "Typing **alexa_dataframe.head()** prints the first 5 entries of the dataframe. _Try it by pressing **SHIFT+ENTER** on the cell_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the Alexa 100k domain list, we're not using the 1 Million just for speed reasons. Results\n",
    "# for the Alexa 1M are given at the bottom of the notebook.\n",
    "alexa_dataframe = pd.read_csv('data/alexa_100k.csv', names=['rank','uri'], header=None, encoding='utf-8')\n",
    "alexa_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Second Level Domain with TLDExtract\n",
    "For this exercise, we are only interested in the **2nd level domain**. For example, we are only interested in 'facebook' in 'www.facebook.com' for analysis.\n",
    "\n",
    "Here is a nice little function that we have written for you to extract the domain using an library called _tldextract_. You will just need to call it in the following exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay for this exercise we need the 2nd level domain and nothing else\n",
    "# We will use TLDextract module to extract this.\n",
    "import tldextract\n",
    "import numpy as np\n",
    "\n",
    "def domain_extract(uri):\n",
    "    ext = tldextract.extract(uri)\n",
    "    if (not ext.suffix):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return ext.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Processing in Dataframe\n",
    "Now let's do some data manipulation! Since we are only interested in the 2nd level domain, we are going to use the **domain_extract()** function to parse the 'uri' column of **alexa_dataframe**.\n",
    "\n",
    "#### What you need to do specifically:\n",
    "1. Extract the **'uri'** column from the **alexa_dataframe**\n",
    "2. For each uri in the column, pass to **domain_extract()** function to get the 2nd level domain **(Hint: Use a For Loop)**.\n",
    "3. Put the extracted 2nd level domains into a new column 'domain' **(Hint: Use a python list)**\n",
    "4. Delete the 'rank' and 'uri' column from **alexa_dataframe** as we no longer need them.\n",
    "\n",
    "#### More hints:\n",
    "One of the reason why Pandas dataframe is a convenient data structure for data manipulation because you can extract an entire column if you know the column name:\n",
    "```python\n",
    "# To extract column 'col' from my_dataframe.\n",
    "my_dataframe['col']\n",
    "```\n",
    "We can also delete an entire column\n",
    "```python\n",
    "# Deleting 'col' from my_dataframe.\n",
    "del my_dataframe['col']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TO DO: Write your Exercise 1 code here\n",
    "alexa_dataframe['domain'] = [ domain_extract(uri) for uri in alexa_dataframe['uri']]\n",
    "del alexa_dataframe['rank']\n",
    "del alexa_dataframe['uri']\n",
    "alexa_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alexa_dataframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's possible we have NaNs from blanklines or whatever\n",
    "alexa_dataframe = alexa_dataframe.dropna()\n",
    "alexa_dataframe = alexa_dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a new column to alexa_dataframe and name it 'class'. Label all entries as 'legit'\n",
    "alexa_dataframe['class'] = 'legit'\n",
    "alexa_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Slicing\n",
    "In machine learning, it is important to split the data into **training set** and **testing set**. In this exercise, you are going to try to split the dataframe into 90% training and 10% hold out. Rename the hold out dataframe as **hold_out_alexa** and the 90% set as **alexa_dataframe**.\n",
    "\n",
    "#### Hint: Dataframe Indexing\n",
    "We have seen earlier how to select a column of dataframe. In the other dimension, we can also select the rows in a dataframe by giving it a numerical:\n",
    "```python\n",
    "# Selecting 3rd row of my_dataframe\n",
    "my_dataframe[2]\n",
    "```\n",
    "\n",
    "We can also grab a range:\n",
    "```python\n",
    "# Selecting a range from row 2 - 10\n",
    "my_dataframe[1:10]\n",
    "# Or up to 10\n",
    "my_dataframe[:10]\n",
    "# Or from 10 onwards\n",
    "my_dataframe[10:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle the data (important for training/testing)\n",
    "alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))\n",
    "alexa_total = alexa_dataframe.shape[0]\n",
    "print 'Total Alexa domains %d' % alexa_total\n",
    "\n",
    "# TO-DO: Hold out 10%\n",
    "hold_out_alexa = alexa_dataframe[int(alexa_total*.9):]\n",
    "alexa_dataframe = alexa_dataframe[:int(alexa_total*.9)]\n",
    "\n",
    "print 'Number of Alexa domains: %d' % alexa_dataframe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alexa_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Data Preparation\n",
    "What you have just tried is a flavor of doing **data preparation** using Pandas Dataframe. That was just the legitimate domains. The code below prepares a dataframe of actual DGA domains and finally concatenates them together into a complete data set for training.\n",
    "\n",
    "Of course keep in mind that we are very pampered here since DGA Domains and Alexa Domains were packaged nicely in '.csv' and '.txt' files for us. The real world is more cruel. Don't be surprised that **majority of the work** is done on extracting these data from different data sources, cleaning out corrupted entries and consolidating them into convenient schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the DGA domains\n",
    "dga_dataframe = pd.read_csv('data/dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')\n",
    "\n",
    "# We noticed that the blacklist values just differ by captilization or .com/.org/.info\n",
    "# <Try map operation>\n",
    "dga_dataframe['domain'] = dga_dataframe.applymap(lambda x: x.split('.')[0].strip().lower())\n",
    "del dga_dataframe['raw_domain']\n",
    "\n",
    "# It's possible we have NaNs from blanklines or whatever\n",
    "dga_dataframe = dga_dataframe.dropna()\n",
    "dga_dataframe = dga_dataframe.drop_duplicates()\n",
    "dga_total = dga_dataframe.shape[0]\n",
    "print 'Total DGA domains %d' % dga_total\n",
    "\n",
    "# Set the class\n",
    "dga_dataframe['class'] = 'dga'\n",
    "\n",
    "# Hold out 10%\n",
    "# <Try data slicing>\n",
    "hold_out_dga = dga_dataframe[int(dga_total*.9):]\n",
    "dga_dataframe = dga_dataframe[:int(dga_total*.9)]\n",
    "\n",
    "print 'Number of DGA domains: %d' % dga_dataframe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dga_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the domains in a big pile!\n",
    "all_domains = pd.concat([alexa_dataframe, dga_dataframe], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering\n",
    "After the data has been extracted, a data scientist must design features that can hopefully help us tell DGA Domains apart from legitimate domains. The first 2 features that we are going to experiment with is simply the **length** and the **entropy** of the domain.\n",
    "\n",
    "#### Food for Thought:\n",
    ">What are some of the features you can come up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a length field for the domain\n",
    "all_domains['length'] = [len(x) for x in all_domains['domain']]\n",
    "\n",
    "# Okay since we're trying to detect dynamically generated domains and short\n",
    "# domains (length <=6) are crazy random even for 'legit' domains we're going\n",
    "# to punt on short domains (perhaps just white/black list for short domains?)\n",
    "all_domains = all_domains[all_domains['length'] > 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Write your own entropy function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grabbed this from Rosetta Code (rosettacode.org)\n",
    "import math\n",
    "from collections import Counter\n",
    " \n",
    "def entropy(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a entropy field for the domain\n",
    "all_domains['entropy'] = [entropy(x) for x in all_domains['domain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_domains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_domains.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets plot some stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boxplots show you the distribution of the data (spread).\n",
    "# http://en.wikipedia.org/wiki/Box_plot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the length and entropy of domains\n",
    "all_domains.boxplot('length','class')\n",
    "pylab.ylabel('Domain Length')\n",
    "all_domains.boxplot('entropy','class')\n",
    "pylab.ylabel('Domain Entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the classes up so we can set colors, size, labels\n",
    "cond = all_domains['class'] == 'dga'\n",
    "dga = all_domains[cond]\n",
    "alexa = all_domains[~cond]\n",
    "plt.scatter(alexa['length'], alexa['entropy'], s=140, c='#aaaaff', label='Alexa', alpha=.2)\n",
    "plt.scatter(dga['length'], dga['entropy'], s=40, c='r', label='DGA', alpha=.3)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Length')\n",
    "pylab.ylabel('Domain Entropy')\n",
    "pylab.show()\n",
    "# Below you can see that our DGA domains do tend to have higher entropy than Alexa on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the types of domains that have entropy higher than 4\n",
    "high_entropy_domains = all_domains[all_domains['entropy'] > 4]\n",
    "print 'Num Domains above 4 entropy: %.2f%% %d (out of %d)' % \\\n",
    "            (100.0*high_entropy_domains.shape[0]/all_domains.shape[0],high_entropy_domains.shape[0],all_domains.shape[0])\n",
    "print \"Num high entropy legit: %d\" % high_entropy_domains[high_entropy_domains['class']=='legit'].shape[0]\n",
    "print \"Num high entropy DGA: %d\" % high_entropy_domains[high_entropy_domains['class']=='dga'].shape[0]\n",
    "high_entropy_domains[high_entropy_domains['class']=='legit'].head()\n",
    "\n",
    "# Looking at the results below, we do see that there are more domains\n",
    "# in the DGA group that are high entropy but only a small percentage\n",
    "# of the domains are in that high entropy range..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_entropy_domains[high_entropy_domains['class']=='dga'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In preparation for using scikit learn we're just going to use\n",
    "# some handles that help take us from pandas land to scikit land\n",
    "\n",
    "# List of feature vectors (scikit learn uses 'X' for the matrix of feature vectors)\n",
    "X = all_domains.as_matrix(['length', 'entropy'])\n",
    "\n",
    "# Labels (scikit learn uses 'y' for classification labels)\n",
    "y = np.array(all_domains['class'].tolist()) # Yes, this is weird but it needs \n",
    "                                            # to be an np.array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest is a popular ensemble machine learning classifier.\n",
    "# http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#\n",
    "import sklearn.ensemble\n",
    "#clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20, compute_importances=True) # Trees in the forest\n",
    "clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can use scikit learn's cross validation to assess predictive performance.\n",
    "scores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=5, n_jobs=4)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wow 96% accurate! At this point we could claim success and we'd be gigantic morons...\n",
    "# Recall that we have ~100k 'legit' domains and only 3.5k DGA domains\n",
    "# So a classifier that marked everything as legit would be about\n",
    "# 96% accurate....\n",
    "\n",
    "# So we dive in a bit and look at the predictive performance more deeply.\n",
    "\n",
    "# Train on a 80/20 split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now plot the results of the 80/20 split in a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels = ['legit', 'dga']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "\n",
    "def plot_cm(cm, labels):\n",
    "    \n",
    "    # Compute percentanges\n",
    "    percent = (cm*100.0)/np.array(np.matrix(cm.sum(axis=1)).T)  # Derp, I'm sure there's a better way\n",
    "    \n",
    "    print 'Confusion Matrix Stats'\n",
    "    for i, label_i in enumerate(labels):\n",
    "        for j, label_j in enumerate(labels):\n",
    "            print \"%s/%s: %.2f%% (%d/%d)\" % (label_i, label_j, (percent[i][j]), cm[i][j], cm[i].sum())\n",
    "\n",
    "    # Show confusion matrix\n",
    "    # Thanks kermit666 from stackoverflow :)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(b=False)\n",
    "    cax = ax.matshow(percent, cmap='coolwarm')\n",
    "    pylab.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    pylab.xlabel('Predicted')\n",
    "    pylab.ylabel('True')\n",
    "    pylab.show()\n",
    "\n",
    "plot_cm(cm, labels)\n",
    "\n",
    "# We can see below that our suspicions were correct and the classifier is\n",
    "# marking almost everything as Alexa. We FAIL.. science is hard... lets go drinking...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well our Mom told us we were still cool.. so with that encouragement we're\n",
    "# going to compute NGrams for every Alexa domain and see if we can use the\n",
    "# NGrams to help us better differentiate and mark DGA domains...\n",
    "\n",
    "# Scikit learn has a nice NGram generator that can generate either char NGrams or word NGrams (we're using char).\n",
    "# Parameters: \n",
    "#       - ngram_range=(3,5)  # Give me all ngrams of length 3, 4, and 5\n",
    "#       - min_df=1e-4        # Minimumum document frequency. At 1e-4 we're saying give us NGrams that \n",
    "#                            # happen in at least .1% of the domains (so for 100k... at least 100 domains)\n",
    "alexa_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-4, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm SURE there's a better way to store all the counts but not sure...\n",
    "# At least the min_df parameters has already done some thresholding\n",
    "counts_matrix = alexa_vc.fit_transform(alexa_dataframe['domain'])\n",
    "alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
    "ngrams_list = alexa_vc.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For fun sort it and show it\n",
    "import operator\n",
    "_sorted_ngrams = sorted(zip(ngrams_list, alexa_counts), key=operator.itemgetter(1), reverse=True)\n",
    "print 'Alexa NGrams: %d' % len(_sorted_ngrams)\n",
    "for ngram, count in _sorted_ngrams[:10]:\n",
    "    print ngram, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're also going to throw in a bunch of dictionary words\n",
    "word_dataframe = pd.read_csv('data/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
    "\n",
    "# Cleanup words from dictionary\n",
    "word_dataframe = word_dataframe[word_dataframe['word'].map(lambda x: str(x).isalpha())]\n",
    "word_dataframe = word_dataframe.applymap(lambda x: str(x).strip().lower())\n",
    "word_dataframe = word_dataframe.dropna()\n",
    "word_dataframe = word_dataframe.drop_duplicates()\n",
    "word_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now compute NGrams on the dictionary words\n",
    "# Same logic as above...\n",
    "dict_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)\n",
    "counts_matrix = dict_vc.fit_transform(word_dataframe['word'])\n",
    "dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
    "ngrams_list = dict_vc.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For fun sort it and show it\n",
    "import operator\n",
    "_sorted_ngrams = sorted(zip(ngrams_list, dict_counts), key=operator.itemgetter(1), reverse=True)\n",
    "print 'Word NGrams: %d' % len(_sorted_ngrams)\n",
    "for ngram, count in _sorted_ngrams[:10]:\n",
    "    print ngram, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use the transform method of the CountVectorizer to form a vector\n",
    "# of ngrams contained in the domain, that vector is than multiplied\n",
    "# by the counts vector (which is a column sum of the count matrix).\n",
    "def ngram_count(domain):\n",
    "    alexa_match = alexa_counts * alexa_vc.transform([domain]).T  # Woot vector multiply and transpose Woo Hoo!\n",
    "    dict_match = dict_counts * dict_vc.transform([domain]).T\n",
    "    print '%s Alexa match:%d Dict match: %d' % (domain, alexa_match, dict_match)\n",
    "\n",
    "# Examples:\n",
    "ngram_count('google')\n",
    "ngram_count('facebook')\n",
    "ngram_count('1cb8a5f36f')\n",
    "ngram_count('pterodactylfarts')\n",
    "ngram_count('ptes9dro-dwacty2lfa5rrts')\n",
    "ngram_count('beyonce')\n",
    "ngram_count('bey666on4ce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute NGram matches for all the domains and add to our dataframe\n",
    "all_domains['alexa_grams']= alexa_counts * alexa_vc.transform(all_domains['domain']).T \n",
    "all_domains['word_grams']= dict_counts * dict_vc.transform(all_domains['domain']).T \n",
    "all_domains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_domains.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the vectorized operations of the dataframe to investigate differences\n",
    "# between the alexa and word grams\n",
    "all_domains['diff'] = all_domains['alexa_grams'] - all_domains['word_grams']\n",
    "#all_domains.sort(['diff'], ascending=True).head(10) #depreciated\n",
    "all_domains.sort_values(by='diff', ascending=True).head(10)\n",
    "# The table below shows those domain names that are more 'dictionary' and less 'web'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_domains.sort(['diff'], ascending=False).head(50) # depreciated\n",
    "all_domains.sort_values(by='diff', ascending=False).head(50)\n",
    "# The table below shows those domain names that are more 'web' and less 'dictionary'\n",
    "# Good O' web...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets plot some stuff!\n",
    "# Here we want to see whether our new 'alexa_grams' feature can help us differentiate between Legit/DGA\n",
    "cond = all_domains['class'] == 'dga'\n",
    "dga = all_domains[cond]\n",
    "legit = all_domains[~cond]\n",
    "plt.scatter(legit['length'], legit['alexa_grams'], s=120, c='#aaaaff', label='Alexa', alpha=.1)\n",
    "plt.scatter(dga['length'], dga['alexa_grams'], s=40, c='r', label='DGA', alpha=.3)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Length')\n",
    "pylab.ylabel('Alexa NGram Matches')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets plot some stuff!\n",
    "# Here we want to see whether our new 'alexa_grams' feature can help us differentiate between Legit/DGA\n",
    "cond = all_domains['class'] == 'dga'\n",
    "dga = all_domains[cond]\n",
    "legit = all_domains[~cond]\n",
    "plt.scatter(legit['entropy'], legit['alexa_grams'],  s=120, c='#aaaaff', label='Alexa', alpha=.2)\n",
    "plt.scatter(dga['entropy'], dga['alexa_grams'], s=40, c='r', label='DGA', alpha=.3)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Entropy')\n",
    "pylab.ylabel('Alexa Gram Matches')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets plot some stuff!\n",
    "# Here we want to see whether our new 'word_grams' feature can help us differentiate between Legit/DGA\n",
    "# Note: It doesn't look quite as good as the Alexa_grams but it might generalize better (less overfit).\n",
    "cond = all_domains['class'] == 'dga'\n",
    "dga = all_domains[cond]\n",
    "legit = all_domains[~cond]\n",
    "plt.scatter(legit['length'], legit['word_grams'],  s=120, c='#aaaaff', label='Alexa', alpha=.2)\n",
    "plt.scatter(dga['length'], dga['word_grams'], s=40, c='r', label='DGA', alpha=.3)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Length')\n",
    "pylab.ylabel('Dictionary NGram Matches')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at which Legit domains are scoring low on the word gram count\n",
    "all_domains[(all_domains['word_grams']==0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay these look kinda weird, lets use some nice Pandas functionality\n",
    "# to look at some statistics around our new features.\n",
    "all_domains[all_domains['class']=='legit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at how many domains that are both low in word_grams and alexa_grams (just plotting the max of either)\n",
    "legit = all_domains[(all_domains['class']=='legit')]\n",
    "max_grams = np.maximum(legit['alexa_grams'],legit['word_grams'])\n",
    "ax = max_grams.hist(bins=80)\n",
    "ax.figure.suptitle('Histogram of the Max NGram Score for Domains')\n",
    "pylab.xlabel('Number of Domains')\n",
    "pylab.ylabel('Maximum NGram Score')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at which Legit domains are scoring low on both alexa and word gram count\n",
    "weird_cond = (all_domains['class']=='legit') & (all_domains['word_grams']<3) & (all_domains['alexa_grams']<2)\n",
    "weird = all_domains[weird_cond]\n",
    "print weird.shape[0]\n",
    "weird.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Epiphany... Alexa really may not be the best 'exemplar' set...  \n",
    "#             (probably a no-shit moment for everyone else :)\n",
    "#\n",
    "# Discussion: If you're using these as exemplars of NOT DGA, then your probably\n",
    "#             making things very hard on your machine learning algorithm.\n",
    "#             Perhaps we should have two categories of Alexa domains, 'legit'\n",
    "#             and a 'weird'. based on some definition of weird.\n",
    "#             Looking at the entries above... we have approx 80 domains\n",
    "#             that we're going to mark as 'weird'.\n",
    "#\n",
    "all_domains.loc[weird_cond, 'class'] = 'weird'\n",
    "print all_domains['class'].value_counts()\n",
    "all_domains[all_domains['class'] == 'weird'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we try our machine learning algorithm again with the new features\n",
    "# Alexa and Dictionary NGrams and the exclusion of the bad exemplars.\n",
    "X = all_domains.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])\n",
    "\n",
    "# Labels (scikit learn uses 'y' for classification labels)\n",
    "y = np.array(all_domains['class'].tolist())\n",
    "\n",
    "# Train on a 80/20 split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now plot the results of the 80/20 split in a confusion matrix\n",
    "labels = ['legit', 'weird', 'dga']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "plot_cm(cm, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hun, well that seem to work 'ok', but you don't really want a classifier\n",
    "# that outputs 3 classes, you'd like a classifier that flags domains as DGA or not.\n",
    "# This was a path that seemed like a good idea until it wasn't...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perhaps we will just exclude the weird class from our ML training\n",
    "not_weird = all_domains[all_domains['class'] != 'weird']\n",
    "X = not_weird.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])\n",
    "\n",
    "# Labels (scikit learn uses 'y' for classification labels)\n",
    "y = np.array(not_weird['class'].tolist())\n",
    "\n",
    "# Train on a 80/20 split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now plot the results of the 80/20 split in a confusion matrix\n",
    "labels = ['legit', 'dga']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "plot_cm(cm, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well it's definitely better.. but haven't we just cheated by removing\n",
    "# the weird domains?  Well perhaps, but on some level we're removing\n",
    "# outliers that are bad exemplars. So to validate that the model is still\n",
    "# doing the right thing lets try our new model prediction on our hold out sets.\n",
    "\n",
    "# First train on the whole thing before looking at prediction performance\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Pull together our hold out set\n",
    "hold_out_domains = pd.concat([hold_out_alexa, hold_out_dga], ignore_index=True)\n",
    "\n",
    "# Add a length field for the domain\n",
    "hold_out_domains['length'] = [len(x) for x in hold_out_domains['domain']]\n",
    "hold_out_domains = hold_out_domains[hold_out_domains['length'] > 6]\n",
    "\n",
    "# Add a entropy field for the domain\n",
    "hold_out_domains['entropy'] = [entropy(x) for x in hold_out_domains['domain']]\n",
    "\n",
    "# Compute NGram matches for all the domains and add to our dataframe\n",
    "hold_out_domains['alexa_grams']= alexa_counts * alexa_vc.transform(hold_out_domains['domain']).T\n",
    "hold_out_domains['word_grams']= dict_counts * dict_vc.transform(hold_out_domains['domain']).T\n",
    "\n",
    "hold_out_domains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of feature vectors (scikit learn uses 'X' for the matrix of feature vectors)\n",
    "hold_X = hold_out_domains.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])\n",
    "\n",
    "# Labels (scikit learn uses 'y' for classification labels)\n",
    "hold_y = np.array(hold_out_domains['class'].tolist())\n",
    "\n",
    "# Now run through the predictive model\n",
    "hold_y_pred = clf.predict(hold_X)\n",
    "\n",
    "# Add the prediction array to the dataframe\n",
    "hold_out_domains['pred'] = hold_y_pred\n",
    "\n",
    "# Now plot the results\n",
    "labels = ['legit', 'dga']\n",
    "cm = confusion_matrix(hold_y, hold_y_pred, labels)\n",
    "plot_cm(cm, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay so on our 10% hold out set of 10k domains about ~100 domains were mis-classified\n",
    "# at this point we're made some good progress so we're going to claim success :)\n",
    "#       - Out of 10k domains 100 were mismarked\n",
    "#       - false positives (Alexa marked as DGA) = ~0.6%\n",
    "#       - about 80% of the DGA are getting marked\n",
    "\n",
    "# Note: Alexa 1M results on the 10% hold out (100k domains) were in the same ballpark \n",
    "#       - Out of 100k domains 432 were mismarked\n",
    "#       - false positives (Alexa marked as DGA) = 0.4%\n",
    "#       - about 70% of the DGA are getting marked\n",
    "\n",
    "# Now were going to just do some post analysis on how the ML algorithm performed.\n",
    "\n",
    "# Lets look at a couple of plots to see which domains were misclassified.\n",
    "# Looking at Length vs. Alexa NGrams\n",
    "fp_cond = ((hold_out_domains['class'] == 'legit') & (hold_out_domains['pred']=='dga'))\n",
    "fp = hold_out_domains[fp_cond]\n",
    "fn_cond = ((hold_out_domains['class'] == 'dba') & (hold_out_domains['pred']=='legit'))\n",
    "fn = hold_out_domains[fn_cond]\n",
    "okay = hold_out_domains[hold_out_domains['class'] == hold_out_domains['pred']]\n",
    "plt.scatter(okay['length'], okay['alexa_grams'], s=100,  c='#aaaaff', label='Okay', alpha=.1)\n",
    "plt.scatter(fp['length'], fp['alexa_grams'], s=40, c='r', label='False Positive', alpha=.5)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Length')\n",
    "pylab.ylabel('Alexa NGram Matches')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking at Length vs. Dictionary NGrams\n",
    "cond = (hold_out_domains['class'] != hold_out_domains['pred'])\n",
    "misclassified = hold_out_domains[cond]\n",
    "okay = hold_out_domains[~cond]\n",
    "plt.scatter(okay['length'], okay['word_grams'], s=100,  c='#aaaaff', label='Okay', alpha=.2)\n",
    "plt.scatter(misclassified['length'], misclassified['word_grams'], s=40, c='r', label='Misclassified', alpha=.3)\n",
    "plt.legend()\n",
    "pylab.xlabel('Domain Length')\n",
    "pylab.ylabel('Dictionary NGram Matches')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misclassified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misclassified[misclassified['class'] == 'dga'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also look at what features the learning algorithm thought were the most important\n",
    "importances = zip(['length', 'entropy', 'alexa_grams', 'word_grams'], clf.feature_importances_)\n",
    "importances\n",
    "\n",
    "# From the list below we see our feature importance scores. There's a lot of feature selection,\n",
    "# sensitivity study, etc stuff that you could do if you wanted at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discussion for how to use the resulting models.\n",
    "# Typically Machine Learning comes in two phases\n",
    "#    - Training of the Model\n",
    "#    - Evaluation of new observations against the Model\n",
    "# This notebook is about exploration of the data and training the model.\n",
    "# After you have a model that you are satisfied with, just 'pickle' it\n",
    "# at the end of the your training script and then in a separate\n",
    "# evaluation script 'unpickle' it and evaluate/score new observations\n",
    "# coming in (through a file, or ZeroMQ, or whatever...)\n",
    "#\n",
    "# In this case we'd have to pickle the RandomForest classifier\n",
    "# and the two vectorizing transforms (alexa_grams and word_grams).\n",
    "# See 'test_it' below for how to use them in evaluation mode.\n",
    "\n",
    "\n",
    "# test_it shows how to do evaluation, also fun for manual testing below :)\n",
    "def test_it(domain):\n",
    "    \n",
    "    _alexa_match = alexa_counts * alexa_vc.transform([domain]).T  # Woot matrix multiply and transpose Woo Hoo!\n",
    "    _dict_match = dict_counts * dict_vc.transform([domain]).T\n",
    "    _X = [len(domain), entropy(domain), _alexa_match, _dict_match]\n",
    "    print '%s : %s' % (domain, clf.predict(_X)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examples (feel free to change these and see the results!)\n",
    "# Try adding your own domains.\n",
    "test_it('google')\n",
    "test_it('google88')\n",
    "test_it('facebook')\n",
    "test_it('1cb8a5f36f')\n",
    "test_it('pterodactylfarts')\n",
    "test_it('ptes9dro-dwacty2lfa5rrts')\n",
    "test_it('beyonce')\n",
    "test_it('bey666on4ce')\n",
    "test_it('supersexy')\n",
    "test_it('yourmomissohotinthesummertime')\n",
    "test_it('35-sdf-09jq43r')\n",
    "test_it('clicksecurity')\n",
    "test_it('actstaydeserveradiostage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "The combination of IPython, Pandas and Scikit Learn let us pull in some junky data, clean it up, plot it, understand it and slap it with some machine learning!\n",
    "\n",
    "Clearly a lot more formality could be used, plotting learning curves, adjusting for overfitting, feature selection, on and on... there are some really great machine learning resources that cover this deeper material. In particular we highly recommend the work and presentations of Olivier Grisel at INRIA Saclay. http://ogrisel.com/\n",
    "\n",
    "Some papers on detecting DGA domains:\n",
    " \n",
    " - S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, “Detecting algorithmically generated malicious domain names,” presented at the the 10th annual conference, New York, New York, USA, 2010, pp. 48–61. [http://conferences.sigcomm.org/imc/2010/papers/p48.pdf]\n",
    " - S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, “Detecting algorithmically generated domain-flux attacks with DNS traffic analysis,” IEEE/ACM Transactions on Networking (TON, vol. 20, no. 5, Oct. 2012.\n",
    " - A. Reddy, “Detecting Networks Employing Algorithmically Generated Domain Names,” 2010.\n",
    " - Z. Wei-wei and G. Qian, “Detecting Machine Generated Domain Names Based on Morpheme Features,” 2013.\n",
    " - P. Barthakur, M. Dahal, and M. K. Ghose, “An Efficient Machine Learning Based Classification Scheme for Detecting Distributed Command & Control Traffic of P2P Botnets,” International Journal of Modern …, 2013."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
